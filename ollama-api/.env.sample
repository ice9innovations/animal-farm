# Ollama LLM API Configuration

# Service Settings
PORT=7782
PRIVATE=true

# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
TEXT_MODEL=llama2
VISION_MODEL=llava-llama3
TEMPERATURE=0.3

PROMPT=Briefly describe this image in a single short sentence.

# Configuration Updates (GitHub-first pattern)
AUTO_UPDATE=true
